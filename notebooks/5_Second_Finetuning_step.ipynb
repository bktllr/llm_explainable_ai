{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c0ccb-d08c-497a-807d-67b4f54bcceb",
   "metadata": {},
   "source": [
    "# Step2 - Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2285bf-d845-4ee7-9bc4-8b405274a397",
   "metadata": {},
   "source": [
    "In the second fine-tuning step (Step 2), we conducted our own SHAP analysis using battery data. The insights derived from this analysis were intended to be universally applicable to batteries and should be transferred to the LLM. The following notebook describes how this second unsupervised finetuning step was conducted.\n",
    "\n",
    "## Start Training\n",
    "To start the training, we have to restart the textgeneration-webui, using the following commands:\n",
    "\n",
    "    conda activate textgen\n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/Step1-model --load-in-8bit\n",
    "\n",
    "Thereby, we load the selected base model (Step1-model) in 8bit as recommended by the text-generation-webui-documentation.\n",
    "\n",
    "After the webui started, the following steps have been executed within the webui:\n",
    "\n",
    "1. Switch to tab \"Training\"\n",
    "2. Give the LoRA-file a new name - in our case \"Step2_adapter\"\n",
    "3. Adapt the Hyperparameters for the training:\n",
    "- Training-epochs: 3\n",
    "- Learning-rate: 3e^-4\n",
    "- LoRA Rank: 8\n",
    "- LoRA Alpha: 16\n",
    "- Batch Size: 128\n",
    "- Micro Batch Size: 4\n",
    "- Cutoff Length: 256\n",
    "- LR Scheduler: linear\n",
    "- Overlap Length: 128\n",
    "- Prefer Newline Cut Length: 128\n",
    "4. Go to the Tab \"Raw text file\"\n",
    "5. Select under \"Text file\" the folder \"Step2-Trainingdata\"\n",
    "6. Click \"Start LoRA Training\"\n",
    "\n",
    "The new LoRA-adapter was saved to the folder \"text-generation-webui/loras/Step2_adapter\".\n",
    "\n",
    "## Create Intermediate-Step2-model\n",
    "We decided to create the new Intermediate-Step1-model by merging the newly trained LORA-adapter (Step2_adapter) with the corresponding base model (Step1-model). Therefore, we executed the following code from this notebook:\n",
    "\n",
    "### Load Base-model, tokenizer, LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d9fa05-305f-49a1-9d4e-b72f5c83338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c533c3-6667-443d-9ad3-4fb259e0f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step1-model/\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a40a11e-65a8-4da6-a022-ad02c11eb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897b046-0f3b-4e9f-86c4-a3ade1ca85cc",
   "metadata": {},
   "source": [
    "### Combine Base-model with LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e11c5-8203-43d0-a17b-20bef7cf8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"/home/jupyter-edt_wise34_tf2/text-generation-webui/loras/Step2_adapter/\", is_trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bce2d0-ceb2-4733-82f2-f38403cea8e9",
   "metadata": {},
   "source": [
    "### Merge Base-model with LoRA-adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1a18e3-ba00-48c4-8af1-8c8dadf77342",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e705ad8-3780-48bc-a733-f746666e1bd8",
   "metadata": {},
   "source": [
    "### Save merged model to text-generation-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c620f8-6263-44e8-b7ab-36a1045daf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step2-model/\", safe_serializetion=True)\n",
    "tokenizer.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step2-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655318c-7b46-40f3-a226-925db2722cbc",
   "metadata": {},
   "source": [
    "The new Step2-model is saved to the folder \"/home/[...]/text-generation-webui/models\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
