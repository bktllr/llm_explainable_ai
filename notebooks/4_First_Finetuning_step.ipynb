{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c0ccb-d08c-497a-807d-67b4f54bcceb",
   "metadata": {},
   "source": [
    "# Step1 - Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2285bf-d845-4ee7-9bc4-8b405274a397",
   "metadata": {},
   "source": [
    "In the initial fine-tuning step (Step 1), our focus was on instilling a foundational comprehension of the topics of SHAP and batteries. The aim was to provide the model with a broad understanding of the specified subjects. The following notebook describes the different fine-tuning steps conducted. Thereby, the conda-kernel \"textgen\" is used.\n",
    "\n",
    "## Start Training\n",
    "To start the training, we have to restart the textgeneration-webui, using the following commands:\n",
    "\n",
    "    conda activate textgen\n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/Llama-2-13b-chat-hf --load-in-8bit\n",
    "\n",
    "Thereby, we load the selected base model (Llama-2-13b-chat-hf) in 8bit as recommended by the text-generation-webui-documentation.\n",
    "\n",
    "After the webui started, the following steps have been executed within the webui:\n",
    "\n",
    "1. Switch to tab \"Training\"\n",
    "2. Give the LoRA-file a new name - in our case \"Step1_adapter\"\n",
    "3. Adapt the Hyperparameters for the training:\n",
    "- Training-epochs: 3\n",
    "- Learning-rate: 3e^-4\n",
    "- LoRA Rank: 8\n",
    "- LoRA Alpha: 16\n",
    "- Batch Size: 128\n",
    "- Micro Batch Size: 4\n",
    "- Cutoff Length: 256\n",
    "- LR Scheduler: linear\n",
    "- Overlap Length: 128\n",
    "- Prefer Newline Cut Length: 128\n",
    "4. Go to the Tab \"Raw text file\"\n",
    "5. Select under \"Text file\" the folder \"Step1-Trainingdata\"\n",
    "6. Click \"Start LoRA Training\"\n",
    "\n",
    "After some minutes the new LoRA-adapter was saved to the folder \"text-generation-webui/loras/Step1_adapter\".\n",
    "\n",
    "## Create Intermediate-Step1-model\n",
    "We decided to create the new Intermediate-Step1-model by merging the newly trained LORA-adapter (Step1_adapter) with the corresponding base model (Llama-2-13b-chat-hf). Therefore, we executed the following code from this notebook:\n",
    "\n",
    "### Load Base-model, tokenizer, LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0d9fa05-305f-49a1-9d4e-b72f5c83338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691570b5-f2d4-479c-86f9-1b7c7f2dea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Llama-2-13b-chat-hf/\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a40a11e-65a8-4da6-a022-ad02c11eb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897b046-0f3b-4e9f-86c4-a3ade1ca85cc",
   "metadata": {},
   "source": [
    "### Combine Base-model with LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a34cac2-a480-47e4-9f3a-c56bb8c517a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"/home/jupyter-edt_wise34_tf2/text-generation-webui/loras/Step1_adapter/\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bce2d0-ceb2-4733-82f2-f38403cea8e9",
   "metadata": {},
   "source": [
    "### Merge Base-model with LoRA-adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1a18e3-ba00-48c4-8af1-8c8dadf77342",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e705ad8-3780-48bc-a733-f746666e1bd8",
   "metadata": {},
   "source": [
    "### Save merged model to text-generation-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1460be4-0810-4ffa-a86a-2dd66a520da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step1-model/\", safe_serializetion=True)\n",
    "tokenizer.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step1-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4d9d2-5772-4b65-98e0-aa75dbf6ffbf",
   "metadata": {},
   "source": [
    "The new Step1-model is saved to the folder \"/home/[...]/text-generation-webui/models\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - Cuda (ipykernel)",
   "language": "python",
   "name": "python3-jr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
