{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399e6d30-4d72-49f5-b06d-2e85d20aa08c",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation\n",
    "The quantitative evaluation consists of the unsupervised perplexity evaluation and the supervised loss evaluation. Both evaluations take place within the text-generation-webui. Thereby, we compare the Llama-2-13b-chat-hf base model, the Step1-model, the Step2-model and the final battery model with each other to see the effect of the different fine-tuning steps.\n",
    "\n",
    "The following notebook describes examplary how the perplexity evaluation and the loss evaluation were conducted for the battery model as the evaluations for the other models were conducted identically. The results of the evaluations can be found at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98451c42-a7e9-4544-9bb4-4a948139e1e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading model into text-generation-webui\n",
    "We used the following commands in the terminal to start the webui and to load the battery model in 8bit into it:\n",
    "\n",
    "    conda activate textgen\n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/battery_model --load-in-8bit\n",
    "    \n",
    "## Perplexity Evaluation\n",
    "Afterwards, we started the Perplexity-Evaluation. Therefore, we conducted the following steps in the webui:\n",
    "\n",
    "1. Go to the tab \"Training\"\n",
    "2. Switch to the tab \"Perplexity evaluation\"\n",
    "3. Select the dataset, you want to evaluate on; in our case: Step1_Evaluation_Battery.txt, Step1_Evaluation_SHAP.txt and Step2_Evaluation_text.txt\n",
    "4. The results of the evaluation, will be added to the table below (Corresponding CSV-File: \"/home/[...]/text-generation-webui/logs/evaluation.csv\")\n",
    "\n",
    "## Loss Evaluation\n",
    "The following steps were conducted in the webui for the loss evaluation:\n",
    "\n",
    "1. Go to the tab \"Training\"\n",
    "2. Stay in the tab \"Train LoRA\"\n",
    "3. Change the Epochs-parameter to 1\n",
    "4. Select the following files within the tab \"Formatted Dataset\"\n",
    "    - Data-Format: Alpaca Format\n",
    "    - Dataset: empty\n",
    "    - Evaluation Dataset: Structured_evaluation_dataset\n",
    "5. The results of the evaluation will be shown in the terminal under \"Evaluation_loss\"\n",
    "\n",
    "## Results\n",
    "\n",
    "After conducting the perplexity-evaluation and loss-evaluation on the different models, we received the following results that are presented in our paper under the fine-tuning section.\n",
    "\n",
    "The initial row illustrates the evaluation outcomes for the Llama-2-13b-Base model, with the second row showcasing results for the intermediate Step1 model after the first fine-tuning step. Similarly, the third row displays results for the intermediate Step2 model after the second fine-tuning step, and the final row presents results for the battery model after the third fine-tuning step. \n",
    "The columns, on the other hand, represent different evaluation documents employed for testing. The first and second columns correspond to the SHAP and battery evaluation documents used to assess the success of fine-tuning step 1. In contrast, the third column pertains to the evaluation document used to evaluate the success of fine-tuning step 2. Given that fine-tuning steps 1 and 2 were executed through an unsupervised approach and the evaluation documents were unstructured, the values in these columns indicate perplexity scores. The fourth column, utilizing the structured JSON-file evaluation document for fine-tuning step 3, involves values that represent evaluation losses.\n",
    "Upon comparing the first and second rows (with fine-tuning step 1 in between), a significant improvement is evident in the main evaluation documents of the first and second columns. Additionally, spill-over effects on the other evaluation categories, even though not targeted, are apparent in the third and fourth columns.\n",
    "\n",
    "Contrasting the results of the second and third rows (with fine-tuning step 2 in between), it is apparent that the impact of fine-tuning step 2 was relatively minimal. This can be attributed to the absence of significant new insights in the SHAP values' analysis during the data preparation phase of fine-tuning step 2. Consequently, all insights discovered here had already been transferred to the LLM during fine-tuning step 1, explaining the positive spill-over effects there.\n",
    "\n",
    "Finally, comparing the third with the fourth row (with fine-tuning step 3 in between), it is clear that the third fine-tuning step had a substantial impact on its main evaluation document in column 4. However, the imperative during this fine-tuning step was to enhance the LLM's performance on the evaluation document in column four while maintaining its performance for the other categories. The table reveals a negligible increase in perplexity values for the other categories, while the improvement in the fourth column is significant.\n",
    "\n",
    "In summary, it can be concluded that fine-tuning steps 1 and 3 had the most substantial impact on improving the LLM, while the impact of fine-tuning step 2 remained relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12a684-1b16-49ee-930b-b3490f0679db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - Cuda (ipykernel)",
   "language": "python",
   "name": "python3-jr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
