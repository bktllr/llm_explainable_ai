{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c0ccb-d08c-497a-807d-67b4f54bcceb",
   "metadata": {},
   "source": [
    "# Step3 - Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2285bf-d845-4ee7-9bc4-8b405274a397",
   "metadata": {},
   "source": [
    "In the final fine-tuning step (Step 3), our attention shifted to the instance level, where the LLM was taught to interpret SHAP values accurately and respond to follow-up queries based on these values. For this purpose, we prepared a structured JSON document following the alpaca-format and employed a supervised learning approach to impart this specialized knowledge to the LLM. The following notebook describes how this supervised finetuning step was conducted.\n",
    "\n",
    "## Start Training\n",
    "To start the training, we have to restart the textgeneration-webui, using the following commands:\n",
    "\n",
    "    conda activate textgen\n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/Step2-model --load-in-8bit\n",
    "\n",
    "Thereby, we load the selected base model (Step2-model) in 8bit as recommended by the text-generation-webui-documentation.\n",
    "\n",
    "After the webui started, the following steps have been executed within the webui:\n",
    "\n",
    "1. Switch to tab \"Training\"\n",
    "2. Give the LoRA-file a new name - in our case \"Step3_adapter\"\n",
    "3. Adapt the Hyperparameters for the training:\n",
    "- Training-epochs: 20\n",
    "- Learning-rate: 3e^-4\n",
    "- LoRA Rank: 8\n",
    "- LoRA Alpha: 16\n",
    "- Batch Size: 128\n",
    "- Micro Batch Size: 4\n",
    "- Cutoff Length: 256\n",
    "- LR Scheduler: linear\n",
    "- Overlap Length: 128\n",
    "- Prefer Newline Cut Length: 128\n",
    "4. Go to the Tab \"Formatted Dataset\"\n",
    "5. Adapt the Dataselection:\n",
    "- Data-Format: alpaca-format\n",
    "- Dataset: Step3_Training_data\n",
    "- Evaluation Dataset: None\n",
    "6. Click \"Start LoRA Training\"\n",
    "\n",
    "The new LoRA-adapter was saved to the folder \"text-generation-webui/loras/Step3_adapter\".\n",
    "\n",
    "## Create battery model\n",
    "We decided to create the final battery model by merging the newly trained LORA-adapter (Step3_adapter) with the corresponding base model (Step2-model). Therefore, we executed the following code from this notebook:\n",
    "\n",
    "### Load Base-model, tokenizer, LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94204c1-ecd1-40a6-9332-02832130a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ccb34-7009-45cb-9727-31ac27aa33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/Step2-model/\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a40a11e-65a8-4da6-a022-ad02c11eb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897b046-0f3b-4e9f-86c4-a3ade1ca85cc",
   "metadata": {},
   "source": [
    "### Combine Base-model with LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4b5d9-6190-4a1c-bb03-62962b0ba814",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"/home/jupyter-edt_wise34_tf2/text-generation-webui/loras/Step3_adapter/\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bce2d0-ceb2-4733-82f2-f38403cea8e9",
   "metadata": {},
   "source": [
    "### Merge Base-model with LoRA-adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1a18e3-ba00-48c4-8af1-8c8dadf77342",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e705ad8-3780-48bc-a733-f746666e1bd8",
   "metadata": {},
   "source": [
    "### Save merged model to text-generation-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4bb8c-ea5c-415e-ab62-491b459d5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/battery_model/\", safe_serializetion=True)\n",
    "tokenizer.save_pretrained(\"/home/jupyter-edt_wise34_tf2/text-generation-webui/models/battery_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd1773-88bb-49fc-8ece-8f28d34a0415",
   "metadata": {},
   "source": [
    "The final battery model is saved to the folder \"/home/[...]/text-generation-webui/models\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
